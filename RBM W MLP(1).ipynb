{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6225c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "import copy\n",
    "\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947e011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "eb39576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1ac4febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = train_data/255\n",
    "norm_test_data = test_data/255\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9bb65cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_2d_data(data):\n",
    "    flatten_data=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        flatten_data.append([])\n",
    "        flatten_data[i] = data[i].flatten() \n",
    "    return np.asarray(flatten_data)\n",
    "\n",
    "norm_train_data = train_data/255\n",
    "norm_test_data = test_data/255\n",
    "\n",
    "norm_train_data = flat_2d_data(norm_train_data)\n",
    "norm_test_data = flat_2d_data(norm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7ba37557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_optimizer():\n",
    "    return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "def build_model_Q4(input_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu',input_dim = input_size, activity_regularizer=l1(0.001)))\n",
    "    model.add(layers.Dense(64, activation='relu', activity_regularizer=l1(0.001)))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer = my_optimizer(),  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.002\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "\n",
    "batchsize = 32\n",
    "num_epochs = 100\n",
    "lrate = keras.callbacks.LearningRateScheduler(step_decay, verbose=0)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "242f7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_auto_encoder(out_dimension):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(4*out_dimension, activation='relu',input_dim = 784))\n",
    "    model.add(layers.Dense(2*out_dimension, activation='relu'))\n",
    "    model.add(layers.Dense(out_dimension, activation='relu',name=\"out_encoders\"))\n",
    "    model.add(layers.Dense(2*out_dimension, activation='relu'))\n",
    "    model.add(layers.Dense(4*out_dimension, activation='relu'))\n",
    "    model.add(layers.Dense(784, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer = \"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "25fc867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_128 (Dense)           (None, 336)               263760    \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 168)               56616     \n",
      "                                                                 \n",
      " out_encoders (Dense)        (None, 84)                14196     \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 168)               14280     \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 336)               56784     \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 784)               264208    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,844\n",
      "Trainable params: 669,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "auto_encoder_model = build_auto_encoder(84)\n",
    "auto_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "52c12480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2400/2400 [==============================] - 13s 5ms/step - loss: 0.3044 - val_loss: 0.2900 - lr: 0.0020\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2859 - val_loss: 0.2854 - lr: 0.0020\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2822 - val_loss: 0.2833 - lr: 0.0020\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 13s 6ms/step - loss: 0.2805 - val_loss: 0.2812 - lr: 0.0020\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2793 - val_loss: 0.2812 - lr: 0.0020\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2784 - val_loss: 0.2794 - lr: 0.0020\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2776 - val_loss: 0.2797 - lr: 0.0020\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2770 - val_loss: 0.2793 - lr: 0.0020\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2767 - val_loss: 0.2789 - lr: 0.0020\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2727 - val_loss: 0.2749 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = auto_encoder_model.fit(norm_train_data, norm_train_data,\n",
    "                    epochs=10,\n",
    "                    batch_size=20,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[early_stop, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c2f92e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 5s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "out_auto_encoder = auto_encoder_model.predict(norm_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "62abe55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = auto_encoder_model.get_layer(\"out_encoders\").output\n",
    "intermediate_model = keras.models.Model(inputs = auto_encoder_model.input,outputs=layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e640054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_train_data = intermediate_model.predict(norm_train_data)\n",
    "encoded_test_data = intermediate_model.predict(norm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e1723be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_133 (Dense)           (None, 128)               10880     \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,786\n",
      "Trainable params: 19,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5480 - accuracy: 0.8246 - val_loss: 0.4489 - val_accuracy: 0.8529 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8601 - val_loss: 0.4407 - val_accuracy: 0.8487 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3918 - accuracy: 0.8644 - val_loss: 0.4052 - val_accuracy: 0.8565 - lr: 0.0020\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3747 - accuracy: 0.8704 - val_loss: 0.3962 - val_accuracy: 0.8627 - lr: 0.0020\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3634 - accuracy: 0.8747 - val_loss: 0.3977 - val_accuracy: 0.8656 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8796 - val_loss: 0.3882 - val_accuracy: 0.8627 - lr: 0.0020\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3427 - accuracy: 0.8819 - val_loss: 0.3897 - val_accuracy: 0.8665 - lr: 0.0020\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3349 - accuracy: 0.8827 - val_loss: 0.3924 - val_accuracy: 0.8643 - lr: 0.0020\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3298 - accuracy: 0.8857 - val_loss: 0.3719 - val_accuracy: 0.8692 - lr: 0.0020\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3000 - accuracy: 0.8950 - val_loss: 0.3681 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2912 - accuracy: 0.8980 - val_loss: 0.3593 - val_accuracy: 0.8759 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.9003 - val_loss: 0.3626 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.9032 - val_loss: 0.3776 - val_accuracy: 0.8711 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2776 - accuracy: 0.9032 - val_loss: 0.3705 - val_accuracy: 0.8745 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2733 - accuracy: 0.9043 - val_loss: 0.3670 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2677 - accuracy: 0.9070 - val_loss: 0.3810 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2648 - accuracy: 0.9086 - val_loss: 0.3755 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2612 - accuracy: 0.9094 - val_loss: 0.3784 - val_accuracy: 0.8745 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2582 - accuracy: 0.9105 - val_loss: 0.3866 - val_accuracy: 0.8737 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2387 - accuracy: 0.9191 - val_loss: 0.3758 - val_accuracy: 0.8769 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2353 - accuracy: 0.9205 - val_loss: 0.3775 - val_accuracy: 0.8782 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model_auto_encoder = build_model_Q4(84)\n",
    "model_auto_encoder.summary()\n",
    "history = model_auto_encoder.fit(encoded_train_data, train_label,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batchsize,\n",
    "                    validation_split = 0.2,\n",
    "                    # validation_data=(norm_test_data, test_label),\n",
    "                    callbacks=[early_stop, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9028ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.3953 - accuracy: 0.8696 - 401ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.8695999979972839\n",
      "\n",
      "Test loss: 0.3953070044517517\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_auto_encoder.evaluate(encoded_test_data,  test_label, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60871cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758103c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fe9897a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6815c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "  \n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name = \"bias\",\n",
    "                                      shape = [self.dense.input_shape[-1]],\n",
    "                                      initializer = \"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b = True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a96fe0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RBM_encoder(out_dimension):\n",
    "    model = keras.Sequential()\n",
    "  \n",
    "    layer0 = layers.Flatten(input_shape=(28*28,1))\n",
    "    layer1 = layers.Dense(784, activation='relu')\n",
    "    layer2 = layers.Dense(4*out_dimension, activation='relu')\n",
    "    layer3 = layers.Dense(2*out_dimension, activation='relu')\n",
    "    layer4 = layers.Dense(out_dimension, activation='relu',name=\"out_encoders\")\n",
    "\n",
    "    model.add(layer0)\n",
    "    model.add(layer1)\n",
    "    model.add(layer2)\n",
    "    model.add(layer3)\n",
    "    model.add(layer4)\n",
    "\n",
    "    model.add(LayerTranspose(layer4, activation='relu'))\n",
    "    model.add(LayerTranspose(layer3, activation='relu'))\n",
    "    model.add(LayerTranspose(layer2, activation='relu'))\n",
    "    model.add(LayerTranspose(layer1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer = \"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b13b82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_34 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 336)               263760    \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 168)               56616     \n",
      "                                                                 \n",
      " out_encoders (Dense)        (None, 84)                14196     \n",
      "                                                                 \n",
      " layer_transpose_24 (LayerTr  (None, 168)              14364     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " layer_transpose_25 (LayerTr  (None, 336)              56952     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " layer_transpose_26 (LayerTr  (None, 784)              264544    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " layer_transpose_27 (LayerTr  (None, 784)              616224    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 952,084\n",
      "Trainable params: 952,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RBM_encoder_model = build_RBM_encoder(84)\n",
    "RBM_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "73d38728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "188/188 [==============================] - 8s 40ms/step - loss: 0.3420 - val_loss: 0.3042 - lr: 0.0020\n",
      "Epoch 2/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2941 - val_loss: 0.2891 - lr: 0.0020\n",
      "Epoch 3/15\n",
      "188/188 [==============================] - 6s 34ms/step - loss: 0.2854 - val_loss: 0.2837 - lr: 0.0020\n",
      "Epoch 4/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2799 - val_loss: 0.2799 - lr: 0.0020\n",
      "Epoch 5/15\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.2769 - val_loss: 0.2773 - lr: 0.0020\n",
      "Epoch 6/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2745 - val_loss: 0.2757 - lr: 0.0020\n",
      "Epoch 7/15\n",
      "188/188 [==============================] - 6s 33ms/step - loss: 0.2730 - val_loss: 0.2740 - lr: 0.0020\n",
      "Epoch 8/15\n",
      "188/188 [==============================] - 7s 38ms/step - loss: 0.2715 - val_loss: 0.2735 - lr: 0.0020\n",
      "Epoch 9/15\n",
      "188/188 [==============================] - 7s 39ms/step - loss: 0.2704 - val_loss: 0.2720 - lr: 0.0020\n",
      "Epoch 10/15\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.2679 - val_loss: 0.2701 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "188/188 [==============================] - 8s 41ms/step - loss: 0.2674 - val_loss: 0.2696 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "188/188 [==============================] - 7s 38ms/step - loss: 0.2670 - val_loss: 0.2695 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2666 - val_loss: 0.2691 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2662 - val_loss: 0.2685 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "188/188 [==============================] - 7s 36ms/step - loss: 0.2658 - val_loss: 0.2681 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = RBM_encoder_model.fit(norm_train_data, norm_train_data,\n",
    "                    epochs=15,\n",
    "                    batch_size=256,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[early_stop, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1651ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = RBM_encoder_model.get_layer(\"out_encoders\").output\n",
    "intermediate_model = keras.models.Model(inputs = RBM_encoder_model.input,outputs=layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a47d22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 5s 3ms/step\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "RBM_encoded_train_data = intermediate_model.predict(norm_train_data)\n",
    "RBM_encoded_test_data = intermediate_model.predict(norm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d2824b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_139 (Dense)           (None, 128)               10880     \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,786\n",
      "Trainable params: 19,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5516 - accuracy: 0.8246 - val_loss: 0.4462 - val_accuracy: 0.8529 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4182 - accuracy: 0.8617 - val_loss: 0.4389 - val_accuracy: 0.8512 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3920 - accuracy: 0.8681 - val_loss: 0.3922 - val_accuracy: 0.8651 - lr: 0.0020\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3716 - accuracy: 0.8747 - val_loss: 0.3759 - val_accuracy: 0.8728 - lr: 0.0020\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3624 - accuracy: 0.8762 - val_loss: 0.4001 - val_accuracy: 0.8650 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3547 - accuracy: 0.8785 - val_loss: 0.3787 - val_accuracy: 0.8684 - lr: 0.0020\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3456 - accuracy: 0.8818 - val_loss: 0.3717 - val_accuracy: 0.8714 - lr: 0.0020\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3378 - accuracy: 0.8858 - val_loss: 0.3643 - val_accuracy: 0.8783 - lr: 0.0020\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3383 - accuracy: 0.8859 - val_loss: 0.3694 - val_accuracy: 0.8726 - lr: 0.0020\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3048 - accuracy: 0.8959 - val_loss: 0.3504 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2991 - accuracy: 0.8980 - val_loss: 0.3394 - val_accuracy: 0.8852 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2941 - accuracy: 0.8992 - val_loss: 0.3418 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2899 - accuracy: 0.9002 - val_loss: 0.3542 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.9004 - val_loss: 0.3579 - val_accuracy: 0.8802 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2834 - accuracy: 0.9036 - val_loss: 0.3433 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2801 - accuracy: 0.9044 - val_loss: 0.3546 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2770 - accuracy: 0.9046 - val_loss: 0.3501 - val_accuracy: 0.8811 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2768 - accuracy: 0.9051 - val_loss: 0.3509 - val_accuracy: 0.8804 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2737 - accuracy: 0.9059 - val_loss: 0.3466 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2545 - accuracy: 0.9136 - val_loss: 0.3445 - val_accuracy: 0.8850 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2526 - accuracy: 0.9137 - val_loss: 0.3392 - val_accuracy: 0.8871 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2494 - accuracy: 0.9151 - val_loss: 0.3419 - val_accuracy: 0.8852 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2479 - accuracy: 0.9159 - val_loss: 0.3448 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2454 - accuracy: 0.9164 - val_loss: 0.3500 - val_accuracy: 0.8866 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2438 - accuracy: 0.9165 - val_loss: 0.3512 - val_accuracy: 0.8844 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2418 - accuracy: 0.9167 - val_loss: 0.3481 - val_accuracy: 0.8863 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2404 - accuracy: 0.9183 - val_loss: 0.3565 - val_accuracy: 0.8843 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2383 - accuracy: 0.9187 - val_loss: 0.3609 - val_accuracy: 0.8840 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2377 - accuracy: 0.9193 - val_loss: 0.3488 - val_accuracy: 0.8849 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2269 - accuracy: 0.9234 - val_loss: 0.3539 - val_accuracy: 0.8852 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2255 - accuracy: 0.9243 - val_loss: 0.3498 - val_accuracy: 0.8863 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "model_RBM_encoder = build_model_Q4(84)\n",
    "model_RBM_encoder.summary()\n",
    "history = model_RBM_encoder.fit(RBM_encoded_train_data, train_label,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batchsize,\n",
    "                    validation_split = 0.2,\n",
    "                    # validation_data=(norm_test_data, test_label),\n",
    "                    callbacks=[early_stop, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3aa26df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3702 - accuracy: 0.8831 - 524ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.8830999732017517\n",
      "\n",
      "Test loss: 0.37016016244888306\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_RBM_encoder.evaluate(RBM_encoded_test_data,  test_label, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbadd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee362be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a0bfb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########3MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a864022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2011a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_optimizer():\n",
    "    return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "def build_model():\n",
    "   # model_RBM_encoder\n",
    "    model = keras.Sequential()\n",
    "    model_RBM_encoder,\n",
    "    model.add(layers.Flatten(input_shape=(28*28, 1)))\n",
    "    model.add(layers.Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
    "    model.add(layers.Dense(64, activation='relu', activity_regularizer=l1(0.001)))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer = my_optimizer(),  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.002\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "406372f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.2580 - accuracy: 0.9150 - val_loss: 0.3943 - val_accuracy: 0.8733 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.2416 - accuracy: 0.9200 - val_loss: 0.3945 - val_accuracy: 0.8739 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.2173 - accuracy: 0.9285 - val_loss: 0.3590 - val_accuracy: 0.8911 - lr: 0.0020\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.2109 - accuracy: 0.9316 - val_loss: 0.3706 - val_accuracy: 0.8862 - lr: 0.0020\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.2097 - accuracy: 0.9315 - val_loss: 0.3566 - val_accuracy: 0.8917 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1999 - accuracy: 0.9355 - val_loss: 0.3842 - val_accuracy: 0.8813 - lr: 0.0020\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1971 - accuracy: 0.9370 - val_loss: 0.3734 - val_accuracy: 0.8899 - lr: 0.0020\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1950 - accuracy: 0.9358 - val_loss: 0.3682 - val_accuracy: 0.8894 - lr: 0.0020\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1869 - accuracy: 0.9401 - val_loss: 0.3791 - val_accuracy: 0.8858 - lr: 0.0020\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9513 - val_loss: 0.3658 - val_accuracy: 0.8940 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1488 - accuracy: 0.9557 - val_loss: 0.3727 - val_accuracy: 0.8914 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1463 - accuracy: 0.9558 - val_loss: 0.3732 - val_accuracy: 0.8906 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.9576 - val_loss: 0.3809 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.9583 - val_loss: 0.3787 - val_accuracy: 0.8928 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.9579 - val_loss: 0.3856 - val_accuracy: 0.8956 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1364 - accuracy: 0.9585 - val_loss: 0.4001 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.9593 - val_loss: 0.3956 - val_accuracy: 0.8918 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1315 - accuracy: 0.9616 - val_loss: 0.3920 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1302 - accuracy: 0.9621 - val_loss: 0.3975 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9700 - val_loss: 0.3945 - val_accuracy: 0.8940 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "batchsize = 256\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "lrate = keras.callbacks.LearningRateScheduler(step_decay, verbose=0)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "myhistory = model.fit(norm_train_data, train_label,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batchsize,\n",
    "                    validation_split = 0.2,\n",
    "                    # validation_data=(norm_test_data, test_label),\n",
    "                    callbacks=[early_stop, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c45fe302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.4280 - accuracy: 0.8906 - 344ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.8906000256538391\n",
      "\n",
      "Test loss: 0.4280080199241638\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(norm_test_data,  test_label, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50eea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
